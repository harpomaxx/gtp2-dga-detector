# GPT2 Fine-Tuning for DGA Detection

This project aims to fine-tune a `GPT2` model for DGA (Domain Generation Algorithm) detection. The `GPT2` model is trained on the `harpomaxx/dga-detection` dataset, which contains labeled domain names as either legitimate or generated by a DGA.

## Dataset

The dataset used for fine-tuning the `GPT2` model is `harpomaxx/dga-detection`. It consists of domain names labeled as legitimate or generated by a DGA. The dataset is preprocessed and tokenized before being fed to the `GPT2` model.

## AI-Dojo Project

This project is part of the `AI-Dojo` project.

## Requirements

The following libraries and tools are required to run the project:

- `Python 3.6` or higher
- `PyTorch`
- `Transformers` library by Hugging Face
- `Datasets` library by Hugging Face
- `Pandas` library

## Usage

1. Install the required libraries and tools using `pip`:

   ```bash
   pip install torch transformers datasets pandas
   ```
2. Clone the repository:

   ```
   git clone `https://github.com/harpomaxx/gpt2-dga-detection.git`
   cd `gpt2-dga-detection`
   ```
3. Fine-tune the GPT2 model for DGA detection by running the following command:

   ```bash
   python train-gpt2-dga-detector.py -e `experiment1` -o `output_dir` -s `save_dir`
   ```
   Replace experiment1 with the name of your experiment.
   Replace output_dir with the desired output directory for training and evaluation.
   Replace save_dir with the directory to save the trained model and tokenizer.

4. Test the model on the validation dataset using the following command:
   ```bash
   python test-gpt2-dga-detector.py
   ```


## License
This project is licensed under the MIT License.

